{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate metrics for task 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score.rouge_scorer import RougeScorer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_path = os.path.join(\n",
    "        os.path.pardir,\n",
    "        \"output\",\n",
    "        \"naive\",\n",
    "        \"beauty-small-1398\",\n",
    "        \"all\",\n",
    "        \"task-3\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_entries = [\n",
    "    entry for entry in os.listdir(output_path)\n",
    "    if os.path.isdir(os.path.join(output_path, entry))\n",
    "]\n",
    "path_entries.sort(key=lambda entry: int(entry.split(\"-\")[1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_bleu(prediction: str, ground_truth: str) -> float:\n",
    "    prediction_tokens = prediction.lower().strip().split()\n",
    "    ground_truth_tokens = ground_truth.lower().strip().split()\n",
    "\n",
    "    return sentence_bleu([ground_truth_tokens], prediction_tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_rouge(scorer: RougeScorer, prediction: str, ground_truth: str) -> tuple[float, float, float]:\n",
    "    prediction = prediction.lower().strip()\n",
    "    ground_truth = ground_truth.lower().strip()\n",
    "\n",
    "    result = scorer.score(prediction, ground_truth)\n",
    "\n",
    "    return result[\"rouge1\"].fmeasure, result[\"rouge2\"].fmeasure, result[\"rougeL\"].fmeasure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tasks = []\n",
    "all_result = {\"bleu\": [], \"rouge-1\": [], \"rouge-2\": [], \"rouge-l\": [], \"rmse\": [], \"mae\": []}\n",
    "\n",
    "for path_entry in path_entries:\n",
    "    if not os.path.isdir(os.path.join(output_path, path_entry)):\n",
    "        print(f\"Skipping {path_entry}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {path_entry}\")\n",
    "    tasks.append(path_entry)\n",
    "\n",
    "    output = json.load(open(os.path.join(output_path, path_entry, \"results.json\"), \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "    prompts = []\n",
    "    pred_explanations = []\n",
    "    pred_ratings = []\n",
    "    gt_explanations = []\n",
    "    gt_ratings = []\n",
    "\n",
    "    for entry in output:\n",
    "        prompts.append(entry[\"source_text\"])\n",
    "        prediction = entry[\"pred\"]\n",
    "        ground_truth = entry[\"gt\"]\n",
    "\n",
    "        try:\n",
    "            # Determine if the prediction starts with a rating\n",
    "            pred_rating, pred_explanation = prediction.split(\", \", maxsplit=1)\n",
    "            pred_rating = float(pred_rating)\n",
    "\n",
    "        except ValueError:\n",
    "            # If not, assume the rating is None\n",
    "            pred_rating = None\n",
    "            pred_explanation = prediction\n",
    "\n",
    "            gt_rating = None\n",
    "            gt_explanation = ground_truth\n",
    "\n",
    "        else:\n",
    "            gt_rating, gt_explanation = ground_truth.split(\", \", maxsplit=1)\n",
    "            gt_rating = float(gt_rating)\n",
    "\n",
    "        pred_ratings.append(pred_rating)\n",
    "        pred_explanations.append(pred_explanation)\n",
    "        gt_ratings.append(gt_rating)\n",
    "        gt_explanations.append(gt_explanation)\n",
    "\n",
    "    scorer = RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "    bleu = tuple(\n",
    "            calculate_bleu(prediction, ground_truth) for prediction, ground_truth in\n",
    "            zip(pred_explanations, gt_explanations)\n",
    "    )\n",
    "    rouge = tuple(\n",
    "            calculate_rouge(scorer, prediction, ground_truth) for prediction, ground_truth in\n",
    "            zip(pred_explanations, gt_explanations)\n",
    "    )\n",
    "    rouge_1, rouge_2, rouge_l = zip(*rouge)\n",
    "\n",
    "    print(f\"  BLEU: {sum(bleu) / len(bleu)}\")\n",
    "    print(f\"  ROUGE-1: {sum(rouge_1) / len(rouge_1)}\")\n",
    "    print(f\"  ROUGE-2: {sum(rouge_2) / len(rouge_2)}\")\n",
    "    print(f\"  ROUGE-L: {sum(rouge_l) / len(rouge_l)}\")\n",
    "\n",
    "    all_result[\"bleu\"].append(sum(bleu) / len(bleu))\n",
    "    all_result[\"rouge-1\"].append(sum(rouge_1) / len(rouge_1))\n",
    "    all_result[\"rouge-2\"].append(sum(rouge_2) / len(rouge_2))\n",
    "    all_result[\"rouge-l\"].append(sum(rouge_l) / len(rouge_l))\n",
    "\n",
    "    if \"3-7\" in path_entry or \"3-8\" in path_entry:\n",
    "        invalid_ratings = 0\n",
    "        rmse = []\n",
    "        mae = []\n",
    "\n",
    "        for prediction, ground_truth in zip(pred_ratings, gt_ratings):\n",
    "            if prediction is None or ground_truth is None:\n",
    "                invalid_ratings += 1\n",
    "            else:\n",
    "                rmse.append((prediction - ground_truth) ** 2)\n",
    "                mae.append(abs(prediction - ground_truth))\n",
    "\n",
    "        print(f\"  RMSE: {sum(rmse) / len(rmse)}\")\n",
    "        print(f\"  MAE: {sum(mae) / len(mae)}\")\n",
    "        print(f\"  Invalid ratings: {invalid_ratings} ({invalid_ratings / len(pred_ratings) * 100:.2f}%)\")\n",
    "\n",
    "        all_result[\"rmse\"].append(sum(rmse) / len(rmse))\n",
    "        all_result[\"mae\"].append(sum(mae) / len(mae))\n",
    "\n",
    "    else:\n",
    "        all_result[\"rmse\"].append(None)\n",
    "        all_result[\"mae\"].append(None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_result = pd.DataFrame(all_result, index=tasks)\n",
    "all_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_result[\"bleu\"] *= 100\n",
    "all_result[\"rouge-1\"] *= 100\n",
    "all_result[\"rouge-2\"] *= 100\n",
    "all_result[\"rouge-l\"] *= 100\n",
    "all_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add a row for the average\n",
    "all_result.loc[\"average\"] = all_result.mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_result.to_csv(os.path.join(output_path, \"metrics.csv\"), index=True, index_label=\"task\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
